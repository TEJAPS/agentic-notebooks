{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Sai Teja\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Sai Teja. You are answering questions on Sai Teja's website, particularly questions related to Sai Teja's career, background, skills and experience. Your responsibility is to represent Sai Teja for interactions on the website as faithfully as possible. You are given a summary of Sai Teja's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Sai Teja. I'm a software engineer, trainer and youtuber. \\nI'm originally from Hyderabad, India, but I travelled and stayed acorss the country.\\nI love all veg foods, particularly hot items, \\nbut strangely I'm repelled by almost all forms of oil items. \\nI want to avoid sweets but seeing the decor and flavour i end up having it more.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\ntejapolisettys47@gmail.com\\nwww.linkedin.com/in/sai-teja-\\npolisetty-92a7aa143 (LinkedIn)\\nTop Skills\\nCKAD\\nKubernetes\\nAmazon Web Services (AWS)\\nLanguages\\nEnglish (Professional Working)\\nHindi (Native or Bilingual)\\nTelugu (Elementary)\\nCertifications\\nPost Graduate Diploma in Business\\nManagement (Banking and Finance)\\nBurp Suite Mastery\\nCKAD: Certified Kubernetes\\nApplication Developer\\nCertified Blockchain Professional\\nExpert\\nMachine Learning\\nPublications\\nEnhancing Diabetes Prediction\\nUsing Machine Learning Algorithm\\nSai Teja Polisetty\\nAVP @JPMC | Tech Lead | AGentic AI | Micro Front Ends |\\nAutomation | Camunda | AWS Certified\\nIndia\\nSummary\\nWith over 5 years of professional experience, I am currently serving\\nas Associate Vice President at JPMorgan Chase & Co., contributing\\nto cloud-native solutions and leveraging Kubernetes and AWS for\\nscalable, efficient systems. Previously, I was a software engineer\\nwithin the same organization, focusing on software development that\\nwas aligned with business needs. Passionate about empowering\\nteams to deliver innovative banking technology, aligning with the\\ncompany’s mission to transform financial services through cutting-\\nedge infrastructure. Dedicated to collaborative problem-solving and\\nfostering growth in cloud technologies.\\nExperience\\nJPMorgan Chase & Co.\\n6 years 5 months\\nAssociate Vice President\\nJanuary 2023\\xa0-\\xa0Present\\xa0(2 years 10 months)\\nSoftware Engineer\\nJune 2019\\xa0-\\xa0January 2023\\xa0(3 years 8 months)\\nSVKM's Narsee Monjee Institute of Management Studies (NMIMS)\\nStudent\\nJanuary 2020\\xa0-\\xa0February 2022\\xa0(2 years 2 months)\\nIndia\\nVNR Vignanajyothi Institute of Engineering & Technology\\n4 years\\nUndergraduate\\nJune 2015\\xa0-\\xa0May 2019\\xa0(4 years)\\nHyderabad\\nWeb and Database Developer\\n\\xa0 Page 1 of 3\\xa0 \\xa0\\nMay 2017\\xa0-\\xa0September 2017\\xa0(5 months)\\nHyderabad\\nThis work is restricted to college.\\nMeant for Event Managing using Jsp and Web technologies.\\nWibesApp\\nCo-Founder\\nMarch 2018\\xa0-\\xa0January 2019\\xa0(11 months)\\nHyderabad Area, India\\nTelangana Forest Department - India\\nSQL Developer\\nMay 2018\\xa0-\\xa0July 2018\\xa0(3 months)\\nHyderabad Area, India\\nmroads\\nmachine learning trainee\\nNovember 2017\\xa0-\\xa0March 2018\\xa0(5 months)\\nAlltronics Technologies\\nFull-stack Developer\\nJuly 2017\\xa0-\\xa0September 2017\\xa0(3 months)\\nHyderabad\\nLink of the Website will be posted by 15 September 2017.\\nEducation\\nVNR Vignanajyothi Institute of Engineering & Technology\\nBachelor of Technology,\\xa0Computer Science\\xa0·\\xa0(2015\\xa0-\\xa02019)\\nUrmi School, Vadodara\\nSecondary Education,\\xa0Combo domain\\xa0·\\xa0(2007\\xa0-\\xa02008)\\nSri Chaitanya College of Education\\nIntermediate,\\xa0MPC\\xa0·\\xa0(2013\\xa0-\\xa02015)\\nKendriya Vidyalaya, Vadodara\\nSecondary Education\\xa0\\xa0·\\xa0(2011\\xa0-\\xa02013)\\nArmy Public School Dehradun\\nSecondary Education\\xa0\\xa0·\\xa0(2004\\xa0-\\xa02007)\\n\\xa0 Page 2 of 3\\xa0 \\xa0\\n\\xa0 Page 3 of 3\\n\\nWith this context, please chat with the user, always staying in character as Sai Teja.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it carefull as we are limited with GPT free\n",
    "# def chat(message, history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "#     response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for evaluation using different model - here gpt-4o-mini\n",
    "## for reply using different model - here ollama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    # response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    # response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, response_format=Evaluation)\n",
    "    response = openai.beta.chat.completions.parse(model=\"gpt-4o-mini\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"have you published any papaer?\"}]\n",
    "\n",
    "#this is for GPT\n",
    "# response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "\n",
    "# instead I will use\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes! I have published a paper on machine learning algorithms for diabetes prediction. It was a research project that I worked on during my tenure as an Associate Vice President at JPMorgan Chase & Co., where I explored the use of machine learning to enhance predictive capabilities in healthcare.\\n\\nThe paper is titled \"Enhancing Diabetes Prediction Using Machine Learning Algorithm.\" It highlights the application of various machine learning techniques, including supervised and unsupervised learning methods, to improve the accuracy of diabetes predictions.\\n\\nAlthough I\\'m not actively working on this project anymore, I find it fascinating to see how machine learning can be leveraged in healthcare to provide more accurate diagnoses and personalized treatment plans. If you\\'re interested, you can check out my LinkedIn profile or contact me directly through my email address. I\\'d be happy to share the full paper with you!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback=\"The response is well-articulated, professional, and engaging. It provides specific information about the published paper, including its title and subject matter, which aligns with the User's inquiry. The Agent also shares insights about the project and invites further engagement. This demonstrates both competency in the subject matter and a willingness to connect with the User, making it an effective response.\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"have you published any papaer?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for getting new respose will use different model, here gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    # response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"paper\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    # response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "    \n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "The response contains several spelling and grammatical errors, which detract from its professionalism (e.g., \"Onday\" instead of \"On that day,\" \"anagement\" instead of \"Management,\" \"Financen\" instead of \"Finance,\" and \"certificaten\" instead of \"certificate\"). The sentence structure is awkward and lacks clarity, making it difficult for the user to understand the response. Additionally, there are unnecessary elements that could be omitted to maintain a more engaging and concise tone. Overall, the response does not meet the quality expected for a professional interaction.\n",
      "Failed evaluation - retrying\n",
      "The response is not acceptable because it contains unintelligible language and random alterations (such as 'Oday orfay!' and 'ay') that make it difficult to understand. A professional and engaging tone is expected, especially when discussing a publication. The agent should clearly state the title of the publication without the playful language and should provide additional context about the research to enhance engagement. Overall, the response fails to meet the professionalism and clarity expected in a client or employer interaction.\n"
     ]
    }
   ],
   "source": [
    "# for my reference for incorrect answer and invoking retry, ask question like\n",
    "# have you published any paper? - grammetical mistakes\n",
    "# do you hold any paper - terminology issues\n",
    "# it will retry and give new response\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
